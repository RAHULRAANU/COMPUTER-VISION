{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40272c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/nachi-hebbar/Transfer-Learning-ResNet-Keras.gitInvalidArgumentError: Graph execution error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a47cb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 10:17:31.172314: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-10 10:17:32.081194: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-10 10:17:32.081250: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-10 10:17:35.041747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-10 10:17:35.042256: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-10 10:17:35.042300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13fcfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16930c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dir = r'/home/ubuntu/Downloads/rajsir_data/Classification_number_data/number_plate data_classification/'\n",
    "\n",
    "train_dir = os.path.join(my_dir, 'train')\n",
    "\n",
    "val_dir =  os.path.join(my_dir, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b789cd",
   "metadata": {},
   "source": [
    "## Find Useless Image\n",
    "  Which Image has graph is not plotted properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e115d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PIL\n",
    "# from pathlib import Path\n",
    "# from PIL import UnidentifiedImageError\n",
    "\n",
    "# path = Path(r'/home/ubuntu/Downloads/cv/number_plate data/train').rglob(\"*.jpg\")\n",
    "# for img_p in path:\n",
    "#     try:\n",
    "#         img = PIL.Image.open(img_p)\n",
    "#     except PIL.UnidentifiedImageError:\n",
    "#             print(img_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4bf5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# IMAGE_SIZE = 224\n",
    "# batch_size = 36\n",
    "\n",
    "# datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# train_ds = datagen.flow_from_directory(train_dir,\n",
    "#                                         target_size = (IMAGE_SIZE,IMAGE_SIZE),\n",
    "#                                         batch_size = batch_size,\n",
    "#                                         class_mode = 'categorical')\n",
    "\n",
    "# val_ds = datagen.flow_from_directory(val_dir,\n",
    "#                                         target_size = (IMAGE_SIZE,IMAGE_SIZE),\n",
    "#                                         batch_size = batch_size,\n",
    "#                                         class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9381d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 359627 files belonging to 36 classes.\n",
      "Using 298491 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 10:18:06.860275: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-10 10:18:06.860316: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-10 10:18:06.860346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu-OptiPlex-3040): /proc/driver/nvidia/version does not exist\n",
      "2023-03-10 10:18:06.868991: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "img_height,img_width = 224,224\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  train_dir,\n",
    "  labels='inferred',  \n",
    "  validation_split = 0.17,\n",
    "  subset = \"training\",\n",
    "  seed = 123,\n",
    "  image_size = (img_height, img_width),\n",
    "  shuffle = True,  \n",
    "  label_mode = 'int',                         # for int use sparse_categorical_crossentropy\n",
    "  batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97767b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87988 files belonging to 36 classes.\n",
      "Using 14957 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  val_dir,\n",
    "  labels='inferred',  \n",
    "  validation_split = 0.17,\n",
    "  subset = \"validation\",\n",
    "  seed = 123,\n",
    "  image_size = (img_height, img_width),\n",
    "  shuffle = True,\n",
    "  label_mode='int',  \n",
    "  batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a91cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
    "# test_datagen  = ImageDataGenerator( rescale = 1.0/255. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2603d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7', 'class_8', 'class_9', 'class_A', 'class_B', 'class_C', 'class_D', 'class_E', 'class_F', 'class_G', 'class_H', 'class_I', 'class_J', 'class_K', 'class_L', 'class_M', 'class_N', 'class_O', 'class_P', 'class_Q', 'class_R', 'class_S', 'class_T', 'class_U', 'class_V', 'class_W', 'class_X', 'class_Y', 'class_Z']\n"
     ]
    }
   ],
   "source": [
    "# class_names = train_ds.class_indices\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5673d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 224, 224, 3)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed36a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COLOR CHANNEL IMAGE\n",
    "\n",
    "# normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47320605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "# image_batch, labels_batch = next(iter(normalized_ds))\n",
    "# first_image = image_batch[0]\n",
    "\n",
    "# # Notice the pixel values are now in `[0,1]`.\n",
    "# print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61896e8d",
   "metadata": {},
   "source": [
    "## Configuring the dataset for better performance\n",
    "Buffered prefetching can be used to ensure that the data can be taken from disk without having I/O become blocking. \n",
    "Dataset.cache() keeps the images in memory after they have been loaded off disk during the first epoch. Dataset.prefetch() will overlap the data preprocessing and model execution while training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c9dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f76e9c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = glob(train_dir + \"/*\")\n",
    "K = len(folders)\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5de7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = Sequential()\n",
    "\n",
    "# pretrained_model= tf.keras.applications.InceptionResNetV2(include_top = False,\n",
    "\n",
    "pretrained_model= tf.keras.applications.ResNet50(include_top = False,\n",
    "#                    input_shape = (224,224,3),        #   + [3], Making the image into 3 Channel, so concating 3.\n",
    "                   pooling = 'max',\n",
    "                   classes = 36,\n",
    "                   weights = 'imagenet')\n",
    "\n",
    "for layer in pretrained_model.layers:                 # pretrained_model.trainable = False\n",
    "        layer.trainable=False\n",
    "\n",
    "resnet_model.add(pretrained_model)\n",
    "resnet_model.add(Flatten())\n",
    "# resnet_model.add(Rescaling(1./255))\n",
    "# resnet_model.add(Dense(512, activation='relu'))\n",
    "# resnet_model.add(Dropout(0.3))\n",
    "# resnet_model.add(BatchNormalization())\n",
    "# resnet_model.add(Dense(256, activation='relu'))\n",
    "# resnet_model.add(Dropout(0.25))\n",
    "# resnet_model.add(BatchNormalization())\n",
    "resnet_model.add(Dense(128, activation='relu'))\n",
    "resnet_model.add(Dropout(0.2))\n",
    "resnet_model.add(BatchNormalization())\n",
    "resnet_model.add(Dense(K, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f2caca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "# model = Sequential()\n",
    "# model.add(VGG16(weights=\"imagenet\", include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation=\"relu\"))\n",
    "# model.add(Dense(128, activation=\"relu\"))\n",
    "# model.add(Dense(64, activation=\"relu\"))\n",
    "# model.add(Dense(4, activation=\"sigmoid\"))\n",
    "\n",
    "# model.layers[-6].trainable = False\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f695a9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " module_wrapper (ModuleWrapp  (None, 2048)             0         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " module_wrapper_1 (ModuleWra  (None, 128)              262272    \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128)              512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " module_wrapper_2 (ModuleWra  (None, 36)               4644      \n",
      " pper)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,855,140\n",
      "Trainable params: 267,172\n",
      "Non-trainable params: 23,587,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2516e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# resnet_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "resnet_model.compile(loss = 'sparse_categorical_crossentropy', optimizer =  Adam(learning_rate=0.01) , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51131dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 2, verbose=0, mode = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701cd2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  899/18656 [>.............................] - ETA: 7:08:06 - loss: 1.7349 - accuracy: 0.4503"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "history = resnet_model.fit(train_ds, epochs = 1, batch_size = batch_size, validation_data = val_ds, callbacks = [earlystop])\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "print(f'Training took: {(stop-start)/60} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=1\n",
    "# history = resnet_model.fit(\n",
    "#   train_ds,\n",
    "#   validation_data=val_ds,\n",
    "#   epochs=epochs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0379f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ebe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "scores = resnet_model.evaluate(val_ds, verbose=0)\n",
    "print(scores)\n",
    "print(\"Score : %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe591226",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = resnet_model.evaluate(val_ds, steps=int(100))\n",
    "\n",
    "print(\"Test results \\n Loss:\",test_loss,'\\n Accuracy',test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca23dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image=cv2.imread('/home/ubuntu/Downloads/rahul/char_165_671604_1674814554496_7.jpg')\n",
    "image_resized= cv2.resize(image, (img_height,img_width))\n",
    "image=np.expand_dims(image_resized,axis=0)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50184990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  from tensorflow.keras.preprocessing import image\n",
    "#     img = image.load_img(path + image_name, target_size = IMAGE_SIZE)\n",
    "#     x = image.img_to_array(img)\n",
    "#     x = x / 255\n",
    "#     x = np.expand_dims(x, axis = 0)\n",
    "#     img_data = preprocess_input(x)\n",
    "#     a = np.argmax(model.predict(img_data), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e473a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = resnet_model.predict(image)\n",
    "\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_class = class_names[np.argmax(Y_pred)]\n",
    "print(\"The predicted class is\", output_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2416987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f83f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred1 = resnet_model.predict(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a2b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = np.argmax(Y_pred, axis=1)\n",
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca38d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(val_ds.classes, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46675cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(validation_generator.classes, y_pred, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d03964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf_matrix = confusion_matrix(train_ds, Y_pred)\n",
    "# conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c9d0b",
   "metadata": {},
   "source": [
    "## Creating an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c605cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    " \n",
    "img = Image.new('RGB', (24, 44), color = (255, 255, 255))\n",
    " \n",
    "# fnt = ImageFont.truetype('/Library/Fonts/Arial.ttf', 15)  # selecting the font\n",
    "d = ImageDraw.Draw(img)\n",
    "d.text((11,11), \"K\", fill = (0, 0, 0))\n",
    " \n",
    "img.save('K.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484f63f",
   "metadata": {},
   "source": [
    "## Extract coordinate from xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c264eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import xml.etree.ElementTree as xet\n",
    "\n",
    "\n",
    "path = glob('/home/ubuntu/Downloads/CVAT/*annotations.xml')\n",
    "\n",
    "labels_dict = dict(FileName=[], Xmin=[], Xmax=[], Ymin=[], Ymax=[], ClassName=[])\n",
    "\n",
    "for filename in path:\n",
    "    print(filename)\n",
    "\n",
    "    info = xet.parse(filename)\n",
    "    print(\"info \",info)\n",
    "    \n",
    "    root = info.getroot()\n",
    "    print(\"root \",root)\n",
    "    \n",
    "    for box in root.findall('./image/box'):\n",
    "        Xmin = box.attrib['xtl']\n",
    "        Xmax = box.attrib['xbr']\n",
    "        Ymin = box.attrib['ytl']\n",
    "        Ymax = box.attrib['ybr']\n",
    "        ClassName = box.attrib['label']\n",
    "#         print(box.attrib)\n",
    "\n",
    "    \n",
    "        for  image in root.iter('image'): \n",
    "            FileName = image.attrib['name']\n",
    "            print(image.attrib)\n",
    "            print(FileName)\n",
    "        labels_dict['FileName'].append(FileName)\n",
    "        labels_dict['Xmin'].append(Xmin)\n",
    "        labels_dict['Xmax'].append(Xmax)\n",
    "        labels_dict['Ymin'].append(Ymin)\n",
    "        labels_dict['Ymax'].append(Ymax)\n",
    "        labels_dict['ClassName'].append(ClassName)\n",
    "        \n",
    "#          print(\"aaa\",labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(labels_dict)\n",
    "df.to_csv('labels.csv',index=False)\n",
    "df\n",
    "\n",
    "# df.to_csv('annotation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b37cc6a",
   "metadata": {},
   "source": [
    "## Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('annotation.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
